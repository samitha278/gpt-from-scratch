{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+m3CYLqytppdqzN+RDuPM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/gpt-from-scratch/blob/main/GPT_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import random"
      ],
      "metadata": {
        "id": "tDPI-mQIHtht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"1ia6z4itw7WJWpnoTohURX6Lm-AnZmVZz\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output = \"input.txt\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "vX2lDjnvIvll",
        "outputId": "00898b60-618e-4df5-c392-ba7d3825a87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ia6z4itw7WJWpnoTohURX6Lm-AnZmVZz\n",
            "To: /content/input.txt\n",
            "100%|██████████| 1.12M/1.12M [00:00<00:00, 9.67MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'input.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nv2qQxTJYH0",
        "outputId": "57fe21a6-c940-45b6-af88-438480d541eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw2xR7QTKFvp",
        "outputId": "7b5cb271-a673-466d-e4fe-70582d7f5680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i,s in enumerate(chars)}\n",
        "\n",
        "encode = lambda s : [stoi[ch] for ch in s]\n",
        "decode = lambda l : ''.join(itos[i] for i in l)\n",
        "\n",
        "print(encode(\"Hello world\"))\n",
        "print(decode(encode(\"Hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cnqtaA5KG4X",
        "outputId": "b9dbf102-ecd6-4c77-ef04-fd652c3228d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
            "Hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text))\n",
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjyancDSRgIp",
        "outputId": "fa286448-e328-4688-8420-20f7d5baccd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9* len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]"
      ],
      "metadata": {
        "id": "gCCeKh6WUFYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "\n",
        "x = train[:block_size]\n",
        "y = train[1:block_size+1]\n",
        "\n",
        "for i in range(block_size):\n",
        "  context = x[:i+1]\n",
        "  target = y[i]\n",
        "\n",
        "  print(f'{context} target:{target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F1f9l90VVvC",
        "outputId": "8dbef70d-1cdb-44de-be59-94768ca57b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18]) target:47\n",
            "tensor([18, 47]) target:56\n",
            "tensor([18, 47, 56]) target:57\n",
            "tensor([18, 47, 56, 57]) target:58\n",
            "tensor([18, 47, 56, 57, 58]) target:1\n",
            "tensor([18, 47, 56, 57, 58,  1]) target:15\n",
            "tensor([18, 47, 56, 57, 58,  1, 15]) target:47\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47]) target:58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(100-block_size, (4,))\n",
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0dSQLVFM4UQ",
        "outputId": "a0b2b4bb-78e7-4d09-ea9e-a4f7a85fb27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([57, 75, 26, 74])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(278)\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "  data = train if split=='train' else val\n",
        "  ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:block_size+i] for i in ix])\n",
        "  y = torch.stack([data[i+1:block_size+i+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "\n",
        "xb,yb"
      ],
      "metadata": {
        "id": "VUqDG0uBcN0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd43db8-80af-448a-8078-037b3ee0bf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[47, 57,  0, 42, 39, 59, 45, 46],\n",
              "         [40, 59, 58,  1, 40, 39, 57, 58],\n",
              "         [58, 53,  1, 46, 43, 56,  1, 57],\n",
              "         [50, 63,  0, 58, 39, 56, 56, 63]]),\n",
              " tensor([[57,  0, 42, 39, 59, 45, 46, 58],\n",
              "         [59, 58,  1, 40, 39, 57, 58, 39],\n",
              "         [53,  1, 46, 43, 56,  1, 57, 53],\n",
              "         [63,  0, 58, 39, 56, 56, 63,  1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigram Language Model"
      ],
      "metadata": {
        "id": "-bdswUyciIta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(278)\n",
        "\n",
        "class BigramLM(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_emb_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "\n",
        "  def __call__(self,idx , targets=None):\n",
        "\n",
        "    logits = self.token_emb_table(idx)    #shape: (b,t,c)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      loss = F.cross_entropy(logits.view(-1,vocab_size) , targets.view(-1))\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self,idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      logits , loss = self(idx)\n",
        "\n",
        "      logits = logits[:,-1,:] #from all batch's last element's logits\n",
        "\n",
        "      probs = F.softmax(logits,dim=1)\n",
        "\n",
        "      ix = torch.multinomial(probs,num_samples=1)\n",
        "\n",
        "      idx = torch.cat((idx,ix), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "bigram = BigramLM(vocab_size)\n",
        "logits , loss = bigram(xb,yb)\n",
        "\n",
        "print(logits.shape,loss)\n",
        "\n",
        "\n",
        "\n",
        "idx= torch.zeros((1,1),dtype= torch.long)\n",
        "\n",
        "print(decode(bigram.generate(idx,max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZcc2HxMHv5l",
        "outputId": "9f9c4017-cfe0-49ee-a596-d1bf0e55a087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8, 65]) tensor(4.5468, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "hDkMQcyOQpP-rU-,VfVk:rXwxj Ug$$kNRxr.x'R3ULl!WC?fErPF'K'nybrlziq:IF:J.-YVN.jj$R-kDwR\n",
            "hWiDAg,rHH'!JzL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([i.shape for i in bigram.parameters()])   #token_emb_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok6jBOBXMMAK",
        "outputId": "51261983-430f-4bb9-a6f9-7907c3d33a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.Size([65, 65])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(bigram.parameters(),lr = 1e-3)"
      ],
      "metadata": {
        "id": "O-8ftb_0kxxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "  xb,yb = get_batch('train')\n",
        "\n",
        "  logits , loss = bigram(xb,yb  )\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  if i%10000 == 0:\n",
        "    print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z18XvwxBqk7J",
        "outputId": "157bf27e-723c-4674-a909-d4d01a3f6e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.7092204093933105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx= torch.zeros((1,1),dtype= torch.long)\n",
        "\n",
        "print(decode(bigram.generate(idx,max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCpboy-Osf_q",
        "outputId": "f7dbb759-1645-4997-e9a3-4e3b6467a374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hW. U EQ&KHb\n",
            "SKB3FKB&jq&p;JoYXMvlik:cusuILA:ivFrod3Y!\n",
            "K?$ne FsT liO't JCoAHpEqKLKm!mLD3fMWArtawnJICl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Averaging past context"
      ],
      "metadata": {
        "id": "szy4_fxo83D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = get_batch('train')\n",
        "x[:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie_2vD6R8hWy",
        "outputId": "00635249-e57d-452c-abb0-22d5b6434b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49,  1, 46, 47, 57,  1, 45, 56],\n",
              "        [ 1, 50, 47, 60, 43,  6,  0, 21],\n",
              "        [61, 56, 39, 54,  1, 53, 59, 56],\n",
              "        [46,  7,  7,  0,  0, 29, 33, 17]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb,yb = bigram(x,y)\n",
        "B,T,C = xb.shape\n",
        "xb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McqmuoSMs8KP",
        "outputId": "154c46bc-c85b-475a-8221-40af5ca485d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros(xb.shape)\n",
        "\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "\n",
        "    xprev = xb[b,:t+1]\n",
        "    xbow[b,t] = xprev.mean(0)"
      ],
      "metadata": {
        "id": "EvqNBUVY8_8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Averaging with matrix multiplication"
      ],
      "metadata": {
        "id": "9zE5JLMNJZQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg8 = torch.tensor([1/i for i in range(1,T+1)])\n",
        "avg8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc4zw8RI98ts",
        "outputId": "784ffd5b-e40f-49d2-ed3f-406a745e99fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 0.5000, 0.3333, 0.2500, 0.2000, 0.1667, 0.1429, 0.1250])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg88 = (avg8*torch.ones(T,T)).T"
      ],
      "metadata": {
        "id": "PyD4iqGcG1X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg88_tril = torch.tril(avg88)\n",
        "avg88_tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym0kyf3_I9E-",
        "outputId": "c5f6350f-2d26-46eb-f5ce-183078c95398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tril(torch.ones(T,T))\n",
        "a = a / a.sum(1,keepdim=True)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwvxD1pXLBl3",
        "outputId": "18139f5b-f119-4d54-cae6-1598925b8746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow_mat = torch.zeros(xb.shape)\n",
        "\n",
        "for batch in range(B):\n",
        "  xbow_mat[batch] = a @ xb[batch]\n"
      ],
      "metadata": {
        "id": "EZ9CAWEwJJrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2 = a @ xb"
      ],
      "metadata": {
        "id": "oLQwX-QFMVSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(xbow,xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkkmgcC3KYPL",
        "outputId": "798ff183-2fb1-4d39-8473-62091ef5bac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of averaging"
      ],
      "metadata": {
        "id": "lhh4vygKNBV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tril(torch.ones(T,T))\n",
        "a = a / a.sum(1,keepdim=True)\n",
        "\n",
        "xbow = a @ xb\n"
      ],
      "metadata": {
        "id": "R7-Q7Pe6M1g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 3"
      ],
      "metadata": {
        "id": "tyz-62DQRov_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "\n",
        "wei = F.softmax(wei,dim = 1)\n",
        "\n",
        "xbow3 = wei @ xb\n",
        "(xbow3 == xbow2).all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Z9p3OuRoWM",
        "outputId": "0623aa22-5f74-4143-c50c-cc40eebbc35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 4 - Self attention"
      ],
      "metadata": {
        "id": "Wo7yxXGNun3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(278)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "opdWkZ9qsDqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ea46c2-b463-40bd-8e76-9fefe14cc57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k1 = torch.randint(20,(8,16))\n",
        "q1 = torch.randint(20,(8,16))\n",
        "q1 @ k1.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9UC1Bn1vFNG",
        "outputId": "6f89c954-25b2-4698-ca80-110cc902d9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2207, 1825, 1972, 2191, 1443, 1603, 1861, 1726],\n",
              "        [1803, 1803, 1540, 1247, 1334, 1368, 1510, 1474],\n",
              "        [2062, 1867, 1510, 1463, 1350, 1605, 1576, 1113],\n",
              "        [2460, 2033, 2103, 1838, 1646, 1975, 1993, 1439],\n",
              "        [2238, 2017, 1827, 1520, 1592, 2044, 1740, 1312],\n",
              "        [1968, 1718, 1644, 1577, 1226, 1635, 1436, 1127],\n",
              "        [2198, 1941, 1831, 1410, 1704, 1656, 1783, 1584],\n",
              "        [2163, 1772, 1826, 1560, 1523, 1418, 1789, 1328]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mat mul high dimension"
      ],
      "metadata": {
        "id": "R0NiIdN7sSHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn((2,4,8))\n",
        "b = torch.randn((2,4,4))\n",
        "b @ a"
      ],
      "metadata": {
        "id": "JRyGfr0qKbvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43346183-ed20-4de7-8cbd-45b5467da43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1080,  3.4493, -0.0389, -2.0157,  0.3596, -1.5135,  1.6457,\n",
              "          -1.3712],\n",
              "         [-1.0791, -0.5694, -0.1386,  2.9522,  0.5635,  0.4879, -1.8812,\n",
              "           3.2153],\n",
              "         [-1.7204, -0.5798,  0.2062,  0.4670,  0.4478, -0.1849, -0.1060,\n",
              "           1.6224],\n",
              "         [-2.2160,  4.4720,  0.0718, -1.4928,  1.2393, -2.3581,  1.6715,\n",
              "           0.7310]],\n",
              "\n",
              "        [[ 3.7422, -0.6046,  1.8760, -1.7515,  1.8201, -2.3911, -2.3626,\n",
              "           0.6900],\n",
              "         [ 3.2248,  0.7812,  2.1391, -0.6751,  1.5220, -2.4509, -3.0965,\n",
              "           1.6815],\n",
              "         [ 3.8133, -1.2514,  2.2410, -2.2957,  2.3149, -2.5146, -0.9141,\n",
              "           0.3985],\n",
              "         [ 2.1424, -1.9206,  4.1068, -1.7650,  2.1677, -1.9662,  5.3301,\n",
              "           0.0766]]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xt,yt = get_batch('train')\n",
        "\n",
        "xt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAvm6jslBQR9",
        "outputId": "163f1df4-55a9-4ff0-adca-79aed4e54386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embd = nn.Embedding(65,40)\n",
        "token_embd.weight.shape\n",
        "\n",
        "out = token_embd(xt)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0u7GD7pC5Uh",
        "outputId": "7ca4140c-afd4-4896-e7fa-b944e33c8910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head = nn.Linear(40,65)\n",
        "lm_head.weight.T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9x51LPjDFAi",
        "outputId": "b0f8a440-d288-42d0-b579-1c823b6f311c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head(out).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azupvkEMLg_l",
        "outputId": "1ce9ab6c-305f-46a7-e7a7-147893f6bbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 8, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 Updated Bigram\n"
      ],
      "metadata": {
        "id": "iLwjnX8uz5GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 64\n",
        "eval_iters = 10000\n",
        "n_embd = 128\n",
        "head_size = n_embd\n",
        "max_iter  = 10000\n",
        "learning_rate = 1e-2"
      ],
      "metadata": {
        "id": "6E-l_4D0-yQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class selfAttentionHead(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        key = self.key(x)\n",
        "        query = self.query(x)\n",
        "\n",
        "        weight = query @ key.transpose(-2,-1) * C**-0.5\n",
        "        weight = weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "        weight = F.softmax(weight,dim=-1)\n",
        "\n",
        "        value = self.value(x)\n",
        "\n",
        "        self.out = weight @ value\n",
        "\n",
        "        return self.out\n",
        "\n",
        "\n",
        "\n",
        "class transformerDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_table = nn.Embedding(vocab_size,n_embd)\n",
        "        self.pos_embd_table = nn.Embedding(block_size,n_embd)\n",
        "\n",
        "        self.sa_head = selfAttentionHead(head_size)\n",
        "\n",
        "        self.lm_head = nn.Linear(head_size,vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,input, targets = None):\n",
        "\n",
        "        B,T = input.shape\n",
        "\n",
        "        token_embd = self.embd_table(input)\n",
        "        pos_embd = self.pos_embd_table(torch.arange(T))\n",
        "\n",
        "        x = token_embd+pos_embd\n",
        "        x = self.sa_head(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T,C),targets.view(-1))\n",
        "\n",
        "        return logits , loss\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self,input,max_token):\n",
        "\n",
        "        for _ in range(max_token):\n",
        "            input_cond = input[:,-block_size:]\n",
        "            logits , loss = self(input_cond)\n",
        "            logits = logits[:,-1,:]\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_index = torch.multinomial(probs,1)\n",
        "\n",
        "            input = torch.cat((input,next_index),dim = 1)\n",
        "\n",
        "        return input\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # create a PyTorch optimizer\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "\n",
        "            xb,yb = get_batch('train')\n",
        "\n",
        "            # evaluate model\n",
        "            logits , loss = model(xb,yb)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if i% (max_iter/10) == 0:\n",
        "                print(f'{i}/{max_iter}  {loss}')\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "\n",
        "# model evaluation\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train model\n",
        "model = transformerDecoder()\n",
        "#model.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# genarate from the model\n",
        "# context = torch.zeros((1,1),dtype=torch.long)\n",
        "# print(decode(model.generate(context,max_token=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "84zZf-riL2hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.__dict__['_modules']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc0vZGwB1awT",
        "outputId": "e19d2c35-251a-4c38-e45a-00994f2872eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'embd_table': Embedding(65, 128),\n",
              " 'pos_embd_table': Embedding(8, 128),\n",
              " 'sa_head': selfAttentionHead(\n",
              "   (key): Linear(in_features=128, out_features=16, bias=False)\n",
              "   (query): Linear(in_features=128, out_features=16, bias=False)\n",
              "   (value): Linear(in_features=128, out_features=16, bias=False)\n",
              " ),\n",
              " 'lm_head': Linear(in_features=16, out_features=65, bias=True)}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "68aLDCPB7Uso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "block_size = 8\n",
        "eval_iters = 10000\n",
        "n_embd = 128\n",
        "head_size = 16\n",
        "max_iter  = 10000\n",
        "learning_rate = 1e-2"
      ],
      "metadata": {
        "id": "EK7U7K-E6ZYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input= torch.tensor([[53, 50, 43, 56, 10,  0, 25, 39]])\n",
        "targets = torch.tensor([50, 43, 56, 10,  0, 25, 39, 42])"
      ],
      "metadata": {
        "id": "9-7YWLPK7kko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model = transformerDecoder()"
      ],
      "metadata": {
        "id": "asRPHFFr9-xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model(input,targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q6u341X-dCn",
        "outputId": "5e8d8995-2449-4650-8a94-229c14f1cc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 3.3355e-01,  6.8891e-01, -8.3918e-01,  1.0558e+00, -6.4635e-01,\n",
              "            7.8033e-01,  5.4743e-01,  1.2486e-01, -6.3534e-02,  1.5087e-01,\n",
              "            5.5966e-01, -7.2753e-02, -1.9249e-01,  1.4256e-01, -4.9183e-03,\n",
              "           -6.4516e-01,  9.2371e-02, -5.7139e-01, -7.9722e-01, -2.4662e-01,\n",
              "           -4.1347e-01,  3.2933e-01,  7.5681e-01, -6.5150e-01, -1.5127e-01,\n",
              "            3.0669e-01, -2.5543e-01, -4.1121e-01, -1.9136e-01, -2.4008e-01,\n",
              "            8.5032e-02, -3.7000e-01,  3.8740e-01,  1.0096e-02, -1.9931e-01,\n",
              "           -1.3735e-02, -1.0452e-01, -2.8312e-01, -3.9280e-01,  4.1761e-01,\n",
              "           -1.1185e-01, -9.2053e-01,  3.0232e-01,  2.2996e-01,  1.1474e-01,\n",
              "            7.1255e-01, -3.1004e-01, -2.5723e-01,  5.8349e-02, -3.9239e-01,\n",
              "            1.0397e+00, -4.0278e-01,  8.5969e-01,  7.7121e-01, -6.2261e-01,\n",
              "           -8.6065e-01, -4.7702e-01, -1.0585e-01,  8.5334e-02, -7.1318e-01,\n",
              "            6.1245e-02, -1.5040e-01,  4.3655e-01, -6.4171e-01,  3.8137e-03],\n",
              "          [ 9.2931e-02,  4.4460e-01, -1.8489e-01,  6.5227e-01, -3.1574e-01,\n",
              "            3.4559e-01,  4.4996e-01, -5.8501e-02, -3.5738e-01,  1.1491e-01,\n",
              "            2.8621e-01, -7.4732e-02, -1.1158e-01, -9.4630e-02, -7.5898e-02,\n",
              "           -3.7868e-01,  1.4198e-01, -2.2599e-01, -3.8791e-01, -1.5120e-01,\n",
              "           -9.1859e-02,  2.0165e-01,  3.8438e-01, -2.1456e-01, -8.1624e-02,\n",
              "            3.3883e-01,  7.4228e-02, -1.9677e-01,  4.8797e-02, -3.7124e-01,\n",
              "           -9.1955e-02, -8.5763e-02,  2.1947e-01,  1.6729e-01, -2.2840e-01,\n",
              "           -1.3241e-01, -1.9607e-01, -1.5283e-01, -2.7442e-01,  1.0548e-01,\n",
              "           -1.0667e-01, -6.4323e-01,  1.6857e-01,  3.1528e-01,  1.3451e-01,\n",
              "            3.0369e-01, -1.0636e-01,  4.1723e-03,  1.8605e-01, -2.1317e-01,\n",
              "            5.9113e-01, -1.3311e-01,  5.4991e-01,  3.9431e-01, -5.7563e-01,\n",
              "           -3.4061e-01, -5.9249e-01, -1.6357e-01, -1.4844e-02, -3.0234e-01,\n",
              "           -8.2649e-02,  1.7593e-03,  1.8595e-01, -3.7524e-01, -6.5713e-03],\n",
              "          [ 5.8140e-02,  1.4013e-01, -1.7634e-01,  5.6619e-01,  8.1702e-02,\n",
              "            4.8947e-02,  2.4310e-01, -4.8139e-01, -3.4624e-01,  1.8550e-01,\n",
              "            2.2565e-01, -1.5039e-01, -2.5703e-01, -1.6945e-01,  1.3153e-01,\n",
              "           -5.8338e-02,  2.9212e-01,  9.1834e-02,  2.4068e-02,  5.9858e-02,\n",
              "           -4.4152e-01, -3.5242e-02, -2.3211e-02, -2.7893e-01, -2.0675e-01,\n",
              "            3.0955e-01, -2.2540e-01,  3.0053e-01, -1.1401e-01, -7.7621e-02,\n",
              "            4.3097e-02,  3.0933e-01,  1.2195e-01, -6.2154e-02, -3.0636e-01,\n",
              "            8.1561e-02, -5.6756e-01, -1.3610e-01, -1.9108e-01,  8.8957e-02,\n",
              "           -4.6231e-01, -5.0774e-01,  6.2143e-02,  3.1967e-01, -1.6205e-01,\n",
              "            1.7331e-01, -6.2891e-02,  1.5802e-01,  9.2044e-02,  9.0847e-03,\n",
              "            3.7193e-01, -2.6598e-02,  7.4156e-01,  1.1962e-01, -3.1929e-01,\n",
              "           -1.4190e-01, -7.7921e-01, -2.8166e-01, -1.9967e-01, -2.1671e-01,\n",
              "           -1.2104e-01,  5.0391e-03,  2.5922e-01, -4.1432e-01,  2.3439e-01],\n",
              "          [ 2.3599e-01, -8.0182e-02, -1.2441e-02,  4.3550e-01,  3.6337e-01,\n",
              "           -9.8168e-02,  2.9004e-01, -5.9183e-01, -4.9962e-01,  1.1020e-01,\n",
              "           -3.8103e-02, -1.7227e-01, -4.9699e-01, -2.0461e-01,  1.6460e-01,\n",
              "            7.5740e-02,  1.4785e-01,  4.6079e-02,  9.2760e-02,  6.4661e-02,\n",
              "           -2.3275e-01, -1.6126e-01, -4.5251e-02, -7.5905e-02,  5.3822e-02,\n",
              "            3.5446e-01, -3.2876e-01,  7.5405e-02, -1.5711e-01,  6.0903e-02,\n",
              "           -2.8767e-01,  3.5736e-01,  1.7749e-01,  2.6342e-02, -4.4277e-01,\n",
              "            1.6027e-01, -6.6656e-01, -1.8706e-01, -2.2444e-01, -2.2366e-01,\n",
              "           -3.6518e-01, -3.7291e-01,  1.4887e-03,  1.7488e-01, -1.8477e-01,\n",
              "           -1.5961e-02, -7.6810e-03,  3.8320e-01,  1.9125e-01, -1.0242e-01,\n",
              "            1.7937e-01, -3.8147e-02,  4.9249e-01,  1.5436e-01, -3.2106e-01,\n",
              "           -3.6954e-02, -5.4865e-01, -3.1036e-01, -2.1275e-01, -1.2152e-01,\n",
              "           -2.9018e-01,  2.2377e-01, -1.3439e-01, -3.7514e-01,  2.2222e-01],\n",
              "          [ 7.0017e-02,  1.9236e-01, -2.4325e-01,  4.0691e-01, -1.6312e-02,\n",
              "            1.8418e-01,  4.2030e-01, -9.7469e-02, -4.4694e-01,  4.2990e-03,\n",
              "            1.6216e-01, -2.0785e-01, -3.7577e-01, -1.3930e-01, -9.7453e-02,\n",
              "           -8.2241e-03, -2.6809e-02, -6.6894e-02, -1.4919e-01, -1.3223e-01,\n",
              "           -4.3624e-02,  3.3835e-02,  9.2653e-02, -2.4416e-01, -4.6819e-02,\n",
              "            1.9881e-01, -3.1221e-02, -7.4125e-02, -2.8427e-04, -1.1731e-01,\n",
              "           -3.3893e-01,  5.5141e-03,  9.4174e-02, -7.2580e-02, -3.7284e-01,\n",
              "           -4.6853e-02, -4.0026e-01, -5.4227e-02, -2.1427e-01, -8.8896e-02,\n",
              "           -1.5371e-01, -5.4827e-01, -2.9954e-02,  1.2330e-01,  3.8086e-02,\n",
              "            1.2098e-01, -1.8522e-01,  1.4699e-01,  2.1194e-01, -2.4753e-01,\n",
              "            2.5616e-01, -3.5782e-02,  5.3338e-01,  2.5420e-01, -2.9470e-01,\n",
              "           -2.4196e-01, -4.5156e-01, -3.3032e-01, -2.9293e-01, -4.6303e-01,\n",
              "           -1.5025e-01,  7.0582e-02,  1.1273e-02, -4.0243e-01,  5.4481e-02],\n",
              "          [ 1.4243e-01,  1.0630e-01, -3.7976e-01,  4.4360e-01, -1.8392e-01,\n",
              "            2.4462e-01,  3.3597e-01, -7.6749e-02, -3.5056e-01,  2.0804e-03,\n",
              "            2.9653e-01, -2.6992e-01, -4.0587e-01, -9.4924e-02,  2.8137e-02,\n",
              "           -2.6981e-02, -7.1496e-02,  6.7813e-02, -8.7205e-02, -1.7715e-01,\n",
              "           -3.1192e-01, -5.5737e-02,  3.2929e-02, -3.8049e-01, -2.7100e-02,\n",
              "            1.2708e-01, -2.0398e-01,  4.0125e-02,  1.3830e-01, -1.8411e-02,\n",
              "           -1.2432e-01,  2.1290e-01,  1.0429e-01, -9.0828e-02, -3.2693e-01,\n",
              "            8.5548e-02, -5.0872e-01,  1.5485e-01, -5.9551e-02,  4.2038e-02,\n",
              "           -1.3674e-02, -4.5294e-01, -8.0077e-04,  1.9278e-01, -4.7782e-02,\n",
              "            1.6026e-01, -2.8258e-01,  1.4812e-01,  2.3905e-01, -1.9482e-01,\n",
              "            1.2741e-01, -1.1335e-01,  5.4271e-01,  2.0456e-01, -1.8706e-01,\n",
              "           -2.9529e-01, -2.7977e-01, -1.9097e-01, -3.9708e-01, -5.7893e-01,\n",
              "           -1.3890e-01,  1.1516e-01,  1.6709e-01, -5.6689e-01,  9.5089e-02],\n",
              "          [ 1.0854e-01,  7.3554e-02, -3.6750e-01,  4.1144e-01, -1.1953e-01,\n",
              "            1.8573e-01,  3.3334e-01, -4.4379e-02, -2.9302e-01, -5.9883e-03,\n",
              "            2.0871e-01, -1.6103e-01, -3.8515e-01, -1.4210e-01, -7.9402e-02,\n",
              "            8.8438e-02, -1.4678e-01,  1.4566e-01, -6.7724e-03, -1.1286e-01,\n",
              "           -1.7643e-01, -1.1608e-01, -3.2812e-02, -3.7309e-01, -1.3270e-01,\n",
              "            1.4351e-01, -1.2537e-01,  1.1051e-01,  9.1990e-02, -4.9283e-02,\n",
              "           -1.3755e-01,  2.5441e-01,  7.8921e-02, -2.0930e-01, -3.8418e-01,\n",
              "            4.1859e-02, -4.2404e-01,  2.1191e-01,  7.4438e-02, -2.2408e-02,\n",
              "           -7.5491e-02, -3.6166e-01, -8.1271e-02,  4.6062e-02, -1.3181e-01,\n",
              "            3.0232e-02, -2.9203e-01,  3.6198e-02,  2.6364e-01, -1.2841e-01,\n",
              "            1.0566e-01, -5.9188e-02,  3.7622e-01,  1.9881e-01, -2.4434e-01,\n",
              "           -2.6755e-01, -3.0916e-01, -3.6048e-01, -3.8916e-01, -3.8234e-01,\n",
              "           -2.3288e-01,  1.0323e-01,  9.6941e-02, -4.2510e-01,  5.9862e-02],\n",
              "          [ 1.9548e-01,  8.2042e-02, -3.4063e-01,  4.2742e-01, -7.6474e-02,\n",
              "            1.4629e-01,  3.2807e-01, -1.2743e-01, -3.3302e-01,  3.9257e-02,\n",
              "            1.8526e-01, -1.7315e-01, -4.4084e-01, -7.7868e-02, -3.2913e-02,\n",
              "            2.8490e-02, -1.0143e-01,  7.9976e-03, -9.7263e-02, -9.2076e-02,\n",
              "           -1.8021e-01, -5.3890e-02,  3.4849e-02, -3.9175e-01, -1.0661e-01,\n",
              "            1.5919e-01, -2.3884e-01,  7.7726e-02,  7.5014e-02,  3.4608e-02,\n",
              "           -1.1358e-01,  2.6674e-01,  1.2969e-01, -1.2123e-01, -3.2349e-01,\n",
              "            1.1327e-01, -4.2836e-01,  2.3368e-01,  2.9789e-02,  7.0261e-02,\n",
              "           -5.1672e-02, -3.3575e-01, -9.4441e-02,  6.9981e-02, -1.5597e-01,\n",
              "            9.0651e-02, -2.0300e-01,  1.2068e-01,  2.0977e-01, -1.7134e-01,\n",
              "            1.3510e-01, -5.9308e-02,  4.7639e-01,  2.5233e-01, -3.0560e-01,\n",
              "           -2.3118e-01, -2.2935e-01, -3.1752e-01, -3.6976e-01, -3.8083e-01,\n",
              "           -1.7211e-01,  1.0011e-01,  1.2852e-01, -4.8264e-01,  7.1967e-02]]],\n",
              "        grad_fn=<ViewBackward0>),\n",
              " tensor(4.0928, grad_fn=<NllLossBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head1 = selfAttentionHead(16)"
      ],
      "metadata": {
        "id": "BoaNPfYD-fP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn((batch_size,block_size,n_embd))    #(1,8,32)"
      ],
      "metadata": {
        "id": "l9_EvUAYA7p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_out = head1(input)"
      ],
      "metadata": {
        "id": "kN8XE7n_BUCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq9a9EB8BV8h",
        "outputId": "56da0b15-ce80-4cdb-d7df-4d47a6c7c243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT test"
      ],
      "metadata": {
        "id": "wgoEIf_pWKZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#hyperparameters\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "n_embd = 64\n",
        "num_heads = 4\n",
        "head_size = n_embd // num_heads\n",
        "eval_iter = 10000\n",
        "max_iter = 10000\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "\n",
        "# #text read\n",
        "# with open('data/input.txt', 'r') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "\n",
        "#get characters from input.txt\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "#encoding and decoding\n",
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i,s in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "\n",
        "# encode text\n",
        "data = encode(text)\n",
        "\n",
        "# train / val split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "\n",
        "# create mini batch\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split=='train' else val_data\n",
        "\n",
        "    idx = torch.randint(len(data)-block_size , (block_size,))\n",
        "\n",
        "    x = torch.tensor([data[i:i+block_size] for i in idx])\n",
        "    y = torch.tensor([data[i+1:block_size+1+i] for i in idx])\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "\n",
        "class SaHead(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd,head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd,head_size,bias = False)\n",
        "\n",
        "        self.value = nn.Linear(n_embd,head_size, bias = False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        key = self.key(x)\n",
        "        query = self.query(x)\n",
        "\n",
        "        weight = query @ key.transpose(-2,-1) * C**-0.5\n",
        "        weight = weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "        weight = F.softmax(weight,dim=-1)\n",
        "\n",
        "        value = self.value(x)\n",
        "\n",
        "        out = weight @ value\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads,head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sa_heads = nn.ModuleList([SaHead(head_size) for i in range(num_heads)])\n",
        "        self.projection = nn.Linear(num_heads * head_size , n_embd)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = torch.cat([sa(x) for sa in self.sa_heads],dim = -1)\n",
        "\n",
        "        out = self.projection(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd,n_embd) # projection\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,num_heads,n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // num_heads\n",
        "\n",
        "        self.heads = MultiHead(num_heads,head_size)\n",
        "        self.mlp = MLP(n_embd)\n",
        "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
        "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # added pre layerNorms\n",
        "        out = self.heads(self.layer_norm1(x)) + x  # residual conn\n",
        "        out = self.mlp(self.layer_norm2(out)) + out  # residual conn\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_table = nn.Embedding(vocab_size,n_embd)\n",
        "        self.pos_embd_table = nn.Embedding(block_size,n_embd)\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            Block(num_heads,n_embd),\n",
        "            Block(num_heads,n_embd),\n",
        "            Block(num_heads,n_embd),\n",
        "            nn.LayerNorm(n_embd)\n",
        "        )\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,input, targets = None):\n",
        "\n",
        "        B,T = input.shape\n",
        "\n",
        "        token_embd = self.embd_table(input)\n",
        "        pos_embd = self.pos_embd_table(torch.arange(T))\n",
        "\n",
        "        x = token_embd+pos_embd\n",
        "\n",
        "        x = self.block(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T,C),targets.view(-1))\n",
        "\n",
        "        return logits , loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # create a PyTorch optimizer\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "\n",
        "            xb,yb = get_batch('train')\n",
        "\n",
        "            # evaluate model\n",
        "            logits , loss = self(xb,yb)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if i% (max_iter/10) == 0:\n",
        "                print(f'{i}/{max_iter}  {loss}')\n",
        "            if i == max_iter-1:\n",
        "                print(f'{max_iter}/{max_iter}  {loss}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self,input,max_token):\n",
        "\n",
        "        for _ in range(max_token):\n",
        "            input_cond = input[:,-block_size:]\n",
        "            logits , loss = self(input_cond)\n",
        "            logits = logits[:,-1,:]\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_index = torch.multinomial(probs,1)\n",
        "\n",
        "            input = torch.cat((input,next_index),dim = 1)\n",
        "\n",
        "        return input\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "model = GPTModel()\n",
        "model.train()\n",
        "\n",
        "\n",
        "\n",
        "# genarate from the model\n",
        "context = torch.zeros((1,1),dtype=torch.long)\n",
        "print(decode(model.generate(context,max_token=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "7NOM6mr9Blfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1bb698-2cb2-4f95-bb9b-68b167e2a610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/10000  4.419777870178223\n",
            "1000/10000  2.17536997795105\n",
            "2000/10000  2.062408685684204\n",
            "3000/10000  2.344139337539673\n",
            "4000/10000  1.889953851699829\n",
            "5000/10000  2.36834979057312\n",
            "6000/10000  2.168095111846924\n",
            "7000/10000  1.7695579528808594\n",
            "8000/10000  2.1250112056732178\n",
            "9000/10000  2.0149948596954346\n",
            "10000/10000  2.1397757530212402\n",
            "\n",
            "Or conwlarga.\n",
            "\n",
            "LAY I blood tcrain. Belioustil all of that wors, contial of the sance,\n",
            "I we Vather, i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1),dtype=torch.long)\n",
        "print(decode(model.generate(context,max_token=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaQdgGB1WN94",
        "outputId": "f5d702cf-d75e-4346-bf2b-bded8d45ad3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Then thither presilme,\n",
            "Shalk; Monatons in the diss, hiner-ver bite,\n",
            "Master, inten that sene a centio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post norm block"
      ],
      "metadata": {
        "id": "ECjwU0A6BATj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,num_heads,n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // num_heads\n",
        "\n",
        "        self.heads = MultiHead(num_heads,head_size)\n",
        "        self.mlp = MLP(n_embd)\n",
        "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
        "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # added pre layerNorms\n",
        "        out = self.layer_norm1(self.heads(x)) + x  # residual conn\n",
        "        out = self.layer_norm2(self.mlp(out)) + out  # residual conn\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "fRfevGZRXyPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout"
      ],
      "metadata": {
        "id": "ukvb0lmqGtaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Dropout(p=0.2)\n",
        "input = torch.randn(20, 16)\n",
        "output = m(input)"
      ],
      "metadata": {
        "id": "L1z7-DO6BISl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input[:2], output[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsih90d8Gxox",
        "outputId": "103f2461-922e-4ea8-d45a-eb0d3cee7576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.6299,  0.1383, -1.1998,  1.5261, -1.9696,  0.2256, -1.5972,  0.9552,\n",
              "          -1.0033,  2.4680, -1.7472,  0.8085, -0.3762, -0.3624, -0.6486, -0.2558],\n",
              "         [ 0.2277, -0.2409, -1.7033, -1.6424, -0.4022, -1.0217, -0.0447, -1.7052,\n",
              "           0.2181,  0.6675,  1.1619,  0.8225,  1.4728, -0.2752,  1.5735,  0.0059]]),\n",
              " tensor([[-0.7873,  0.1729, -1.4997,  1.9076, -2.4620,  0.2821, -0.0000,  1.1940,\n",
              "          -1.2541,  0.0000, -2.1840,  0.0000, -0.4702, -0.0000, -0.8108, -0.3197],\n",
              "         [ 0.2846, -0.3011, -0.0000, -2.0530, -0.5028, -1.2771, -0.0559, -2.1315,\n",
              "           0.2727,  0.8343,  1.4524,  1.0282,  1.8410, -0.3440,  1.9669,  0.0073]]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add dropout to gpt"
      ],
      "metadata": {
        "id": "mNCayqGSHzvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#hyperparameters\n",
        "block_size = 32\n",
        "batch_size = 64\n",
        "n_embd = 128\n",
        "num_heads = 4\n",
        "num_blocks = 6\n",
        "head_size = n_embd // num_heads\n",
        "eval_iter = 10000\n",
        "max_iter = 10000\n",
        "lr = 1e-4\n",
        "dropout = 0.16\n",
        "\n",
        "\n",
        "\n",
        "# #text read\n",
        "# with open('data/input.txt', 'r') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "\n",
        "#get characters from input.txt\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "#encoding and decoding\n",
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i,s in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "\n",
        "# encode text\n",
        "data = encode(text)\n",
        "\n",
        "# train / val split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "\n",
        "# create mini batch\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split=='train' else val_data\n",
        "\n",
        "    idx = torch.randint(len(data)-block_size , (block_size,))\n",
        "\n",
        "    x = torch.tensor([data[i:i+block_size] for i in idx])\n",
        "    y = torch.tensor([data[i+1:block_size+1+i] for i in idx])\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "\n",
        "class SaHead(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd,head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd,head_size,bias = False)\n",
        "\n",
        "        self.value = nn.Linear(n_embd,head_size, bias = False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        key = self.key(x)\n",
        "        query = self.query(x)\n",
        "\n",
        "        weight = query @ key.transpose(-2,-1) * C**-0.5\n",
        "        weight = weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "        weight = F.softmax(weight,dim=-1)\n",
        "\n",
        "\n",
        "        weight = self.dropout(weight)\n",
        "        value = self.value(x)\n",
        "\n",
        "        out = weight @ value\n",
        "\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads,head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sa_heads = nn.ModuleList([SaHead(head_size) for i in range(num_heads)])\n",
        "        self.projection = nn.Linear(num_heads * head_size , n_embd)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = torch.cat([sa(x) for sa in self.sa_heads],dim = -1)\n",
        "\n",
        "        out = self.projection(out)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd,n_embd), # projection\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        out = self.mlp(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,num_heads,n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // num_heads\n",
        "\n",
        "        self.heads = MultiHead(num_heads,head_size)\n",
        "        self.mlp = MLP(n_embd)\n",
        "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
        "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # added pre layerNorms\n",
        "        out = self.heads(self.layer_norm1(x)) + x  # residual conn\n",
        "        out = self.mlp(self.layer_norm2(out)) + out  # residual conn\n",
        "\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_table = nn.Embedding(vocab_size,n_embd)\n",
        "        self.pos_embd_table = nn.Embedding(block_size,n_embd)\n",
        "\n",
        "        self.block = nn.Sequential(*[Block(num_heads,n_embd) for i in range(num_blocks)])\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,input, targets = None):\n",
        "\n",
        "        B,T = input.shape\n",
        "\n",
        "        token_embd = self.embd_table(input)\n",
        "        pos_embd = self.pos_embd_table(torch.arange(T))\n",
        "\n",
        "        x = token_embd+pos_embd\n",
        "\n",
        "        x = self.block(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T,C),targets.view(-1))\n",
        "\n",
        "        return logits , loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # create a PyTorch optimizer\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
        "\n",
        "        losses = torch.zeros(max_iter)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "\n",
        "            xb,yb = get_batch('train')\n",
        "\n",
        "            # evaluate model\n",
        "            logits , loss = self(xb,yb)\n",
        "\n",
        "            losses[i] = loss\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if i% (max_iter/10) == 0:\n",
        "                print(f'{i}/{max_iter}  {loss}')\n",
        "            if i == max_iter-1:\n",
        "                print(f'{max_iter}/{max_iter}  {loss}')\n",
        "\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self,input,max_token):\n",
        "\n",
        "        for _ in range(max_token):\n",
        "            input_cond = input[:,-block_size:]\n",
        "            logits , loss = self(input_cond)\n",
        "            logits = logits[:,-1,:]\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_index = torch.multinomial(probs,1)\n",
        "\n",
        "            input = torch.cat((input,next_index),dim = 1)\n",
        "\n",
        "        return input\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "model = GPTModel()\n",
        "#losses = model.train()\n",
        "\n",
        "\n",
        "\n",
        "# genarate from the model\n",
        "context = torch.zeros((1,1),dtype=torch.long)\n",
        "print(decode(model.generate(context,max_token=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCkUIKQuGywW",
        "outputId": "4b17dc9a-ad32-4dd5-97c5-ad9271ac93c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "T.:tqam ;\n",
            "TLT HubnbWsFCa'kJi OyH;d-iRFbxCZ;omRGHp.P.-EFZNxqCRjIf.itEK&,3IBOypj.TC.zPeD HIAsMXXHEOhwo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([p.numel() for p in model.parameters()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a97hMgSJYSB1",
        "outputId": "1a05edd3-829b-48e7-b2ca-ccd32a75bb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1208385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gpt on GPU"
      ],
      "metadata": {
        "id": "343gwXvh3XT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#hyperparameters\n",
        "block_size = 32\n",
        "batch_size = 64\n",
        "n_embd = 128\n",
        "num_heads = 4\n",
        "num_blocks = 6\n",
        "head_size = n_embd // num_heads\n",
        "eval_iter = 10000\n",
        "max_iter = 10000\n",
        "lr = 1e-4\n",
        "dropout = 0.16\n",
        "\n",
        "\n",
        "\n",
        "# #text read\n",
        "# with open('data/input.txt', 'r') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "\n",
        "#get characters from input.txt\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "#encoding and decoding\n",
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i,s in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "\n",
        "\n",
        "# encode text\n",
        "data = encode(text)\n",
        "\n",
        "# train / val split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "\n",
        "# create mini batch\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split=='train' else val_data\n",
        "\n",
        "    idx = torch.randint(len(data)-block_size , (block_size,))\n",
        "\n",
        "    x = torch.tensor([data[i:i+block_size] for i in idx])\n",
        "    y = torch.tensor([data[i+1:block_size+1+i] for i in idx])\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "\n",
        "class SaHead(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd,head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd,head_size,bias = False)\n",
        "\n",
        "        self.value = nn.Linear(n_embd,head_size, bias = False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        key = self.key(x)\n",
        "        query = self.query(x)\n",
        "\n",
        "        weight = query @ key.transpose(-2,-1) * C**-0.5\n",
        "        weight = weight.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
        "        weight = F.softmax(weight,dim=-1)\n",
        "\n",
        "\n",
        "        weight = self.dropout(weight)\n",
        "        value = self.value(x)\n",
        "\n",
        "        out = weight @ value\n",
        "\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads,head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sa_heads = nn.ModuleList([SaHead(head_size) for i in range(num_heads)])\n",
        "        self.projection = nn.Linear(num_heads * head_size , n_embd)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = torch.cat([sa(x) for sa in self.sa_heads],dim = -1)\n",
        "\n",
        "        out = self.projection(out)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd,n_embd), # projection\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        out = self.mlp(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,num_heads,n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // num_heads\n",
        "\n",
        "        self.heads = MultiHead(num_heads,head_size)\n",
        "        self.mlp = MLP(n_embd)\n",
        "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
        "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # added pre layerNorms\n",
        "        out = self.heads(self.layer_norm1(x)) + x  # residual conn\n",
        "        out = self.mlp(self.layer_norm2(out)) + out  # residual conn\n",
        "\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_table = nn.Embedding(vocab_size,n_embd)\n",
        "        self.pos_embd_table = nn.Embedding(block_size,n_embd)\n",
        "\n",
        "        self.block = nn.Sequential(*[Block(num_heads,n_embd) for i in range(num_blocks)])\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,input, targets = None):\n",
        "\n",
        "        B,T = input.shape\n",
        "\n",
        "        token_embd = self.embd_table(input)\n",
        "        pos_embd = self.pos_embd_table(torch.arange(T, device=device))\n",
        "\n",
        "        x = token_embd+pos_embd\n",
        "\n",
        "        x = self.block(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T,C),targets.view(-1))\n",
        "\n",
        "        return logits , loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # create a PyTorch optimizer\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
        "\n",
        "        losses = torch.zeros(max_iter)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "\n",
        "            xb,yb = get_batch('train')\n",
        "\n",
        "            # evaluate model\n",
        "            logits , loss = self(xb,yb)\n",
        "\n",
        "            losses[i] = loss\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if i% (max_iter/10) == 0:\n",
        "                print(f'{i}/{max_iter}  {loss}')\n",
        "            if i == max_iter-1:\n",
        "                print(f'{max_iter}/{max_iter}  {loss}')\n",
        "\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate(self,input,max_token):\n",
        "\n",
        "        for _ in range(max_token):\n",
        "            input_cond = input[:,-block_size:]\n",
        "            logits , loss = self(input_cond)\n",
        "            logits = logits[:,-1,:]\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_index = torch.multinomial(probs,1)\n",
        "\n",
        "            input = torch.cat((input,next_index),dim = 1)\n",
        "\n",
        "        return input\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "model = GPTModel()\n",
        "model = model.to(device)\n",
        "losses = model.train()\n",
        "\n",
        "\n",
        "\n",
        "# genarate from the model\n",
        "context = torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "print(decode(model.generate(context,max_token=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "3Z3mCPbZYxGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b87d5f-5a26-4319-8bd3-5ea301b9843f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/10000  4.360275745391846\n",
            "1000/10000  2.4701366424560547\n",
            "2000/10000  2.394174814224243\n",
            "3000/10000  2.253653049468994\n",
            "4000/10000  2.2769970893859863\n",
            "5000/10000  2.2080271244049072\n",
            "6000/10000  2.155518054962158\n",
            "7000/10000  2.076859951019287\n",
            "8000/10000  2.0508880615234375\n",
            "9000/10000  2.046293258666992\n",
            "10000/10000  2.072751760482788\n",
            "\n",
            "Far you.\n",
            "\n",
            "DUCKE?\n",
            "SIO:\n",
            "Shat so bolich he is sort com of it a lam haplame\n",
            "Lard ray:\n",
            "A Gode callerZy a'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses.view(-1,10).mean(1).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "8Izg_S_s6F7G",
        "outputId": "fac22e7c-58fb-4a78-e6fb-97a45098654f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7e9bf34ff680>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([p.numel() for p in model.parameters()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-r1AcQq7z52",
        "outputId": "adc1a544-2897-4e32-a39f-74967edc767c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1208385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ICBT94ftEvFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}