# Generatively Pretrained Transformer from scratch

Implementing a Generatively Pretrained Transformer (GPT) from scratch in PyTorch, inspired by Attention is All You Need paper and OpenAIâ€™s GPT-2/3. Covers the full pipeline of autoregressive language modeling, training, sampling, and evaluation.









## Bigram Language Model (Bigram.py)
### Outout:
0/10000  4.5462846755981445
1000/10000  3.6757185459136963
2000/10000  3.053956985473633
3000/10000  2.7299282550811768
4000/10000  2.556368827819824
5000/10000  2.520214796066284
6000/10000  2.5889151096343994
7000/10000  2.4034910202026367
8000/10000  2.3354899883270264
9000/10000  2.4603657722473145


### Generate 100 tokens:
Fours thid J ous?
Bouelllllighapan ITh.
I s.
LI:
The;

Fary be be bu uneDortanethethe
Foan tha
nchee




