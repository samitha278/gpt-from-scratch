# Generatively Pretrained Transformer from scratch

Implementing a Generatively Pretrained Transformer (GPT) from scratch in PyTorch, inspired by Attention is All You Need paper and OpenAIâ€™s GPT-2/3. Covers the full pipeline of autoregressive language modeling, training, sampling, and evaluation.




#### Transformer Decoder
Next....  





##### Bigram Language Model (Bigram.py)
###### Outout:
0/10000  4.5462846755981445    <br/>
1000/10000  3.6757185459136963 <br/>
2000/10000  3.053956985473633  <br/>
3000/10000  2.7299282550811768  <br/>
4000/10000  2.556368827819824   <br/>
5000/10000  2.520214796066284   <br/>
6000/10000  2.5889151096343994 <br/>
7000/10000  2.4034910202026367 <br/>
8000/10000  2.3354899883270264 <br/>
9000/10000  2.4603657722473145 <br/>


###### Generate 100 tokens: (kinda garbage, need to improve this model)
Fours thid J ous?          <br/> 
Bouelllllighapan ITh.      <br/>
I s.                       <br/>
LI:                        <br/>
The;                       <br/>
                           <br/>
Fary be be bu uneDortanethethe      <br/>
Foan tha                            <br/>
nchee                               <br/>








