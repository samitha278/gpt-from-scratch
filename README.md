# Generatively Pretrained Transformer from scratch

Implementing a Generatively Pretrained Transformer (GPT) from scratch in PyTorch, inspired by Attention is All You Need paper and OpenAIâ€™s GPT-2/3. Covers the full pipeline of autoregressive language modeling, training, sampling, and evaluation.
